{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Lx_NGGfG8YuVP-JE_eqM_xg5FKAcoe-e","timestamp":1664832218556},{"file_id":"1_puLeqSBcK1DShLbWNpgbleRMeehP7JH","timestamp":1657731827573},{"file_id":"1RaqNm3h7krxXhYxfKLFDZ4Sk73AZOWIj","timestamp":1643919938457},{"file_id":"1hKndPITN_ohiRRW0T0JYxvrHLTtCPANv","timestamp":1643644272126}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0KKC3O3QiIi4"},"source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","#ENGG*6600: Special Topics in Information Retrieval - Fall 2022\n","##Assignment 1: Information Retrieval Metrics (Total : 100 points)\n","\n","**Description**\n","\n","This is a coding assignment where you will write and execute code to evaluate ranked outputs generated by a retrieval model or a recommender system. Basic proficiency in Python is recommended.  \n","\n","**Instructions**\n","\n","* To start working on the assignment, you would first need to save the notebook to your local Google Drive. For this purpose, you can click on *Copy to Drive* button. You can alternatively click the *Share* button located at the top right corner and click on *Copy Link* under *Get Link* to get a link and copy this notebook to your Google Drive.  \n","\n","*   For questions with descriptive answers, please replace the text in the cell which states \"Enter your answer here!\" with your answer. If you are using mathematical notation in your answers, please define the variables.\n","*   You should implement all the functions yourself and should not use a library or tool for computing the metrics.\n","*   For coding questions, you can add code where it says \"enter code here\" and execute the cell to print the output.\n","* To create the final pdf submission file, execute *Runtime->RunAll* from the menu to re-execute all the cells and then generate a PDF using *File->Print->Save as PDF*. Make sure that the generated PDF contains all the codes and printed outputs before submission. You are responsible for uploading the correct pdf with all the information required for grading.\n","To create the final python submission file, click on File->Download .py.\n","\n","\n","**Submission Details**\n","\n","* Due data: Sep. 26, 2022 at 11:59 PM (EST).\n","* The final PDF and python file must be uploaded on dropbox at CourseLink.\n","* After copying this notebook to your Google Drive, please paste a link to it below. Use the same process given above to generate a link. ***You will not recieve any credit if you don't paste the link!*** Make sure we can access the file.\n","***LINK: https://colab.research.google.com/drive/1Jv-NzbhSkzzvPz1P-V9rhNI1YwhoHhXt?usp=sharing\n","\n","**Academic Honesty**\n","\n","Please follow the guidelines under the *Collaboration and Help* section in the first lecture.     "]},{"cell_type":"code","source":[],"metadata":{"id":"2M3FfEPyvcAO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JuVqvXU6ijXi"},"source":["# Download input files"]},{"cell_type":"markdown","metadata":{"id":"SMU5d1a-jCG-"},"source":["Please execute the cell below to download the input files."]},{"cell_type":"code","metadata":{"id":"YM1igiBMcIB8"},"source":["\n","import os\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","\n","import os\n","import zipfile\n","\n","download = drive.CreateFile({'id': '1myaSouVnJygjLlQI54_S0vRXLNYekEC8'})\n","download.GetContentFile('HW01.zip')\n","\n","with zipfile.ZipFile('HW01.zip', 'r') as zip_file:\n","    zip_file.extractall('./')\n","os.remove('HW01.zip')\n","# We will use hw1 as our working directory\n","os.chdir('HW01')\n","\n","#Setting the input files\n","qrel_file = \"antique-train-final.qrel\"\n","rank_file = \"ranking_file\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EWB3Hw6UKiPd"},"source":["# 1: Initial Data Setup (10 Points)\n","\n","We use the files from the ANTIQUE dataset [https://arxiv.org/pdf/1905.08957.pdf] for this assignment. This is a passage retrieval dataset for non-factoid questions created by the Center for Intelligent Information Retrieval (CIIR) at UMass Amherst.\n","\n","The description of the input files provided for this assignment is given below.\n","\n","**1) Query Relevance (qrel) file**\n","\n","The qrel file contains the relevance judgements (ground truth) for the query passage combinations. The file consists of 4 columns with the information given below.\n","\n","*\\[queryid]  [topicid]  [passageid]  [relevancejudgment]*\n","\n","Entries in each row are space separated. The second column (topicid) can be ignored.\n","\n","Given below are a couple of rows of a sample qrel file.\n","\n","*2146313 U0 2146313_0 4*\n","\n","*2146313 Q0 2146313_23 2*\n","\n","The relevance judgements range from values 1-4.\n","The description of the labels is given below:\n","\n","Label 1: Non-Relevant\n","\n","Label 2: Slightly Relevant\n","\n","Label 3 : Relevant\n","\n","Label 4: Highly Relevant\n","\n","**Note**: that for metrics with binary relevance assumptions, Labels 1 and 2 are considered non-relevant and Labels 3 and 4 are considered relevant.\n","\n","**Note**: if a query-document pair is not listed in the qrels file, we assume that the document is not relevant to the query.\n","\n","**2) Ranking file**\n","\n","The evaluation metric value has to be calculated for the input ranking file. The file was generated using a standard search engine by executing a ranking model, retrieving the top 100 passages for each of the train queries of the ANTIQUE dataset. The format of this file is given below.\n","\n","*\\[queryid]  [topicid]  [passageid]  [rank] [relevance_score]  [indri]*\n","\n","Similar to the qrel file, the entries in each row are space delimited.\n","\n","Given below are some sample examples of the ranking file contents.\n","\n","*3097310 Q0 2367043_3 1 -6.01785 indri*\n","\n","*3097310 Q0 3007432_0 2 -6.22531 indri*\n","\n","*3097310 Q0 674672_2 3 -6.28514 indri*\n","\n","**Note**: For this assignment, you would only need the information from Column 1(queryid) and Column 3(passageid). The passages corresponding to each query is ranked with respect to the relevance score (highest to lowest), therefore you would not need to use Column 4 (rank) explicitly.\n","\n","\n"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","#first creating dataframes for qrel and rank matrix\n","qrel_dataframe=pd.read_csv(\"antique-train-final.qrel\", names=['q_id','topic_id','p_id','relevance'], sep=' ')\n","rank_dataframe=pd.read_csv(\"ranking_file\", names=['q_id','topic_id','p_id','rank_id','relevance_score','indri'], sep=' ')"],"metadata":{"id":"rOh3j9A_w57x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In order to make it easier to access this information in subsequent cells, please store them in appropriate data structures in the cell below."],"metadata":{"id":"BoWda3QKIiPr"}},{"cell_type":"code","metadata":{"id":"KgJPt-BpKi0q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667432845084,"user_tz":240,"elapsed":23,"user":{"displayName":"Harsh Shah","userId":"05727327381693060940"}},"outputId":"cace0ae5-cfe3-4168-f521-0351dbda96fb"},"source":["\n","'''\n","In this function, load the qrel file into qrel datastructure\n","Return Variables:\n","num_queries_1 - Number of unique queries in the qrel file\n","num_rel - Number of total relevant passages in the dataset across all queries\n","qrels - datastructure with the query passage relevance information\n","'''\n","def loadQrels(qrel_file):\n","      #qrels datastructure(dataframe) from qrel datframe\n","      qrels=qrel_dataframe.drop(columns=\"topic_id\")\n","\n","      #finding number of unique queries\n","      num_queries_1=qrels.q_id.unique().size\n","\n","      #finding number of passages with relevance score more than 3\n","      num_rel=len(qrels.loc[(qrels['relevance']==4) | (qrels['relevance']==3)])\n","\n","      return num_queries_1, num_rel, qrels\n","\n","'''\n","In this function, load the ranking files into rank_in datastructure\n","Return Variables:\n","num_queries_2 - Number of unique queries in the ranking file\n","rank_in - datastructure with stored ranking information\n","'''\n","def  loadRankfile(rank_file):\n","      #enter your code here\n","      #enter your code here\n","      rank_in= rank_dataframe.drop(columns=[\"topic_id\",\"relevance_score\",\"indri\"])\n","\n","      num_queries_2=rank_in.q_id.unique().size\n","\n","      return num_queries_2, rank_in\n","\n","\n","\n","''' You can return single/multiple variables to store data if that makes it convenient\n","for data processing.\n","This has been given as an example. However, you would still need to correctly print the\n","queries in both files and total relevant passages.'''\n","num_queries_1, num_rel, qrels  = loadQrels(qrel_file)\n","num_queries_2, rank_in = loadRankfile(rank_file)\n","\n","# print to ensure the file has been read correctly\n","print ('Total Num of queries in the qrel file  : {0}'.format(num_queries_1))\n","print ('Total Num of queries in the rank file  : {0}'.format(num_queries_2))\n","print ('Total number of relevant passages in the dataset :{0}'.format(num_rel))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Num of queries in the qrel file  : 2426\n","Total Num of queries in the rank file  : 2426\n","Total number of relevant passages in the dataset :19813\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZmtxE4uPrN-6"},"source":["\n","\n","# 2 : Precision (15 Points)\n"]},{"cell_type":"markdown","metadata":{"id":"2BUa5rRXruIG"},"source":["Question 2.1 (5 points)\n","\n","Definition of Precision corresponding to the top *k* (P@k):\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wFvJUJRur9k6"},"source":["**Answer:**\n","\n","Precision is the fraction of the **documents retrieved that are relevant to the user's information need**.\n","\n","For the Precision at k [P(k)] is the fraction of the relevant documents from the top k retrived documents(n) and total number(k) retrieved documents.\n","\n","\n","> **Precision@k**    \n","=   Relevant Retrieved @k / Total Retrieved @ k   \n","                     =   n / k\n","\n","Where,\n","**n**= the number of Relevant Retrieved documents till k,\n","**k**= the number of Total Retrieved at k\n","https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)\n"]},{"cell_type":"markdown","metadata":{"id":"UOE4d9MqsM2M"},"source":["\n","\n","Question 2.2 (10 points)\n","\n","In the cell below, please enter the code to print the P@k where k={5,10} for the input ranking file.  As mentioned above, please note that the final value is the average of metric values across all queries.\n"]},{"cell_type":"code","metadata":{"id":"GqYBuEEisfdv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667432845383,"user_tz":240,"elapsed":309,"user":{"displayName":"Harsh Shah","userId":"05727327381693060940"}},"outputId":"f49169a5-9b03-4253-d5c5-8476e5b26ae4"},"source":["'''\n","In this function, calculate Precision@k, given the input ranking information (rank_in)\n","and the query passage relevance information (qrels).\n","Return Value:\n","precision - Precision@k\n","'''\n","def calcPrecision(k, qrels, rank_in):\n","      #first finding relevant document according to k value\n","      basic=pd.Series(rank_in['q_id'].unique()).reset_index(name='q_id').drop(columns=\"index\")\n","      rank_filtered=rank_in[rank_in.rank_id<=k]\n","      qrels_relevant=qrels[qrels.relevance>=3]\n","\n","      #creating relevant document counts with respect to each query id\n","      relevant_retrived=rank_filtered[rank_filtered['p_id'].isin(qrels_relevant['p_id'])]\n","      relevant_count=relevant_retrived['q_id'].value_counts().rename_axis('q_id').reset_index(name='relevant_count')\n","      ans_a_pre=pd.merge(basic, relevant_count, how=\"outer\", on='q_id').fillna(0)\n","\n","      basic=basic.merge(ans_a_pre)\n","      basic['precision']= basic['relevant_count']/k\n","\n","      precision= basic['precision'].mean()\n","\n","      return precision\n","\n","print ('Precision at top 5 : {0}'.format(calcPrecision(5, qrels, rank_in)))\n","print ('Precision at top 10 : {0}'.format(calcPrecision(10, qrels, rank_in)))\n","\n",""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision at top 5 : 0.20263808738664466\n","Precision at top 10 : 0.15704863973619126\n"]}]},{"cell_type":"markdown","metadata":{"id":"j55h8jAhtkcP"},"source":["# 3 : Recall (15 points)"]},{"cell_type":"markdown","metadata":{"id":"rgR_VxrTmtLn"},"source":["Question 3.1 (5 points)\n","\n","Give the definition of Recall corresponding to the top *k* retrieved results for *n* queries (R@k). Please note that you have to use averaging to aggregate the results from all queries."]},{"cell_type":"markdown","metadata":{"id":"3X1ufq8cmyzo"},"source":["Recall is the fraction of the **documents that are relevant to the query that are successfully retrieved.**\n","\n","For the Recall at k [P(k)] is the fraction of the relevant documents from the top k retrived documents(n) and total retrieved documents(k).\n","\n","\n",">\n","\t         Recall@k   =   Relevant Retrieved @k /  Total Possible Relevant\n","\n","                        =   n / r\n","\n","Where\n","\n","\n","> **n**= the number of Relevant Retrieved documents till k,\n","\n","> **k**= the number of Total Relevant in database for that query\n","\n","\n","\n","\n","\n","\n","Sources: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)\n"]},{"cell_type":"markdown","metadata":{"id":"gQi0Kg4Vm3pn"},"source":["Question 3.2 (10 points)\n","\n","In the cell below, please enter the code to print Recall (R@k) where k={5,10} for the input ranking file. As mentioned above, please note that the final value is the average of metric values across all queries."]},{"cell_type":"code","metadata":{"id":"3SOXEvzloJpG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664841311415,"user_tz":240,"elapsed":16,"user":{"displayName":"Harsh Shah","userId":"05727327381693060940"}},"outputId":"1d056c81-2894-463d-f992-87de1ceed275"},"source":["'''\n","In this function, calculate Recall@k, given the input ranking information (rank_in)\n","and the query passage relevance information (qrels).\n","Return Value:\n","recall - Recall@k\n","'''\n","def calcRecall(k, qrels, rank_in):\n","\n","    basic=pd.Series(rank_in['q_id'].unique()).reset_index(name='q_id').drop(columns=\"index\")\n","    #finding relevant documents\n","    qrels_relevant=qrels[qrels.relevance>=3]\n","\n","    #first finding relevant document according to k value\n","    rank_filtered=rank_in[rank_in.rank_id<=k]\n","    relevant_retrived=rank_filtered[rank_filtered['p_id'].isin(qrels_relevant['p_id'])]\n","    relevant_count=relevant_retrived['q_id'].value_counts().rename_axis('q_id').reset_index(name='relevant_count')\n","    ans_a_pre=pd.merge(basic, relevant_count, how=\"outer\", on='q_id').fillna(0)\n","\n","    #recall calculation\n","    z=qrels_relevant['q_id'].value_counts().rename_axis('q_id').reset_index(name='total_relevant')\n","    recall_counts=pd.merge(ans_a_pre, z, on=\"q_id\")\n","    recall_counts['recall']=recall_counts['relevant_count']/recall_counts['total_relevant']\n","\n","    basic=basic.merge(recall_counts)\n","    recall=recall_counts['recall'].mean()\n","\n","    return recall\n","\n","print ('Recall at top 5 : {0}'.format(calcRecall(5, qrels, rank_in)))\n","print ('Recall at top 10 : {0}'.format(calcRecall(10, qrels, rank_in)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Recall at top 5 : 0.17867080409742425\n","Recall at top 10 : 0.2734828274489789\n"]}]},{"cell_type":"markdown","metadata":{"id":"Z-QYucscmdlK"},"source":["# 4 : F1 Measure (15 Points)"]},{"cell_type":"markdown","metadata":{"id":"_-dG5UL-nem5"},"source":["Question 4.1 (5 points)\n","\n","Give the definition of F1 measure corresponding to the top *k* retrieved results for *n* queries (F1@k). Please note that you have to use averaging to aggregate the results from all queries."]},{"cell_type":"markdown","metadata":{"id":"A_Ptg39nnlNt"},"source":["**Answer:**\n","\n","The harmonic mean of the precision@k and recall@k is called as F1 measure at k.\n","\n","\n","\n",">F1@k   =  1/P@k+1/R@k    \n","\t      =  (2*P@k*R@k)/(P@k+R@k)\n","\n"," https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"W5q6oMPYnnyB"},"source":["Question 4.2 (10 points)\n","\n","In the cell below, please enter the code to print the F1@k where k={5,10} for the input ranking file.  Please note that you have to calculate F1 score for each query and compute the final score by averaging the metric values across all queries."]},{"cell_type":"code","metadata":{"id":"79UYWkerHiwv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664841311416,"user_tz":240,"elapsed":13,"user":{"displayName":"Harsh Shah","userId":"05727327381693060940"}},"outputId":"6ac6d63b-ded0-4a7c-d1f5-145ab2a26d24"},"source":["'''\n","In this function, calculate F1@k, given the input ranking information (rank_in)\n","and the query passage relevance information (qrels).\n","Return Value:\n","f1 - F1@k\n","'''\n","\n","def calcFScore(k, qrels, rank_in):\n","\n","    basic=pd.Series(rank_in['q_id'].unique()).reset_index(name='q_id').drop(columns=\"index\")\n","    #finding relevant documents\n","    qrels_relevant=qrels[qrels.relevance>=3]\n","\n","    #fnding relevant first k documents\n","    rank_filtered=rank_in[rank_in.rank_id<=k]\n","    relevant_retrived=rank_filtered[rank_filtered['p_id'].isin(qrels_relevant['p_id'])]\n","    relevant_count=relevant_retrived['q_id'].value_counts().rename_axis('q_id').reset_index(name='relevant_count')\n","    ans_a_pre=pd.merge(basic, relevant_count, how=\"outer\", on='q_id').fillna(0)\n","\n","    #recall calculation\n","    z=qrels_relevant['q_id'].value_counts().rename_axis('q_id').reset_index(name='total_relevant')\n","    recall_counts=pd.merge(ans_a_pre, z, on=\"q_id\")\n","    recall_counts['recall']=recall_counts['relevant_count']/recall_counts['total_relevant']\n","    basic=basic.merge(recall_counts)\n","\n","\n","    #precision calculation\n","    basic['precision']= basic['relevant_count']/k\n","\n","    #f1score calculation\n","    basic['f1']=(2*(basic['precision']*basic['recall'])/(basic['precision']+basic['recall'])).fillna(0)\n","    f1=basic['f1'].mean()\n","\n","    return f1\n","\n","print ('F1 score at top 5 : {0}'.format(calcFScore(5, qrels, rank_in)))\n","print ('F1 score at top 10 : {0}'.format(calcFScore(10, qrels, rank_in)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 score at top 5 : 0.16780895628714962\n","F1 score at top 10 : 0.17655764871188323\n"]}]},{"cell_type":"markdown","metadata":{"id":"inXXPsyinO4A"},"source":["# 5 : Mean Reciprocal Rank (MRR) (15 Points)"]},{"cell_type":"markdown","metadata":{"id":"MJGcfaWQKazF"},"source":["Question 5.1 (5 points)\n","\n","Give the definition of MRR@k corresponding to the top *k* retrieved results for *n* queries (MRR@k). Please note that you have to use averaging to aggregate the results from all queries."]},{"cell_type":"markdown","metadata":{"id":"juSenU8JLCl6"},"source":["**Answer:**\n","For evaluating the possible outcomes for al**l the queries produced of a process in ordered by the possibility of righteousness is called the Mean Reciprocal Rank.**\n","The mean of the summation of the reciprocal of the first relevant retrieved document in each query for the top k retrieved documents is MRR@k.\n","\n","MRR@k=   1/Q *    ∑   (1/ranki)\n","\n","Sources:\n","https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xz9tZETfLI6c"},"source":["Question 5.2 (10 points)\n","\n","In the cell below, please enter the code to print the MRR@k where k={5,10} for the input ranking file. As mentioned above, please note that the final value is the average of metric values across all queries."]},{"cell_type":"markdown","source":[],"metadata":{"id":"TifY4IO0QOJt"}},{"cell_type":"code","metadata":{"id":"50_vq1o5LMf6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664841311629,"user_tz":240,"elapsed":221,"user":{"displayName":"Harsh Shah","userId":"05727327381693060940"}},"outputId":"a588c2b8-c139-4267-ba98-dbbf3bb45894"},"source":["'''\n","In this function, calculate MRR@k, given the input ranking information (rank_in)\n","and the query passage relevance information (qrels).\n","Return Value:\n","mrr - MRR@k\n","'''\n","\n","def calcMRR(k, qrels, rank_in):\n","     #filtering first k documents\n","    filtered=rank_in[rank_in.rank_id<=k]\n","    da=pd.merge(filtered,qrels,how='left').fillna(0)\n","    #finding relevant documents for query id\n","    db=da.loc[da['relevance']>=3]\n","    #finding first relevant document rank\n","    dc=db.loc[db.groupby('q_id').rank_id.idxmin()]\n","    mrr=(1/dc['rank_id']).mean()\n","\n","    return mrr\n","\n","print ('MRR at top 5 : {0}'.format(calcMRR(5, qrels, rank_in)))\n","print ('MRR at top 10 : {0}'.format(calcMRR(10, qrels, rank_in)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MRR at top 5 : 0.7228743748161224\n","MRR at top 10 : 0.6435193263055858\n"]}]},{"cell_type":"markdown","metadata":{"id":"HypgYsTAPcON"},"source":["# 6 : Mean Average Precision (MAP) (15 points)"]},{"cell_type":"markdown","metadata":{"id":"Fy69CA4wPinF"},"source":["Question 6.1 (5 points)\n","\n","Give the definition of MAP@k corresponding to the top *k* retrieved results for *n* queries (MAP@k). Please note that you have to use averaging to aggregate the results from all queries."]},{"cell_type":"markdown","metadata":{"id":"_Y3FDKGiPopO"},"source":["**Answer:**\n","Mean average precision at k is defined as **the average precision of all the queries for the top k retrieved results divide by the number of all the queries.**\n","\n","MAP@k   =     1/Q * ∑ AP@k\n","\n","where,\n","**Q** is the total number of quries\n","\n","AP@k is the average precision for the top k documents for all queries\n","\n"]},{"cell_type":"markdown","metadata":{"id":"brN7ozRePtd-"},"source":["Question 6.2 (10 points)\n","\n","In the cell below, please enter the code to print the MAP@k where k={50, 100} for the input ranking file. As mentioned above, please note that the final value is the average of metric values across all queries.\n"]},{"cell_type":"code","metadata":{"id":"Sgj5MTwyP6iC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664841311785,"user_tz":240,"elapsed":162,"user":{"displayName":"Harsh Shah","userId":"05727327381693060940"}},"outputId":"6ec0bf4a-4360-43e9-c721-6dbed245aeeb"},"source":["'''\n","In this function, calculate MAP@k, given the input ranking information (rank_in)\n","and the query passage relevance information (qrels).\n","Return Value:\n","map - MAP@k\n","'''\n","\n","def calcMAP(k, qrels, rank_in):\n","  #filtering first k documents\n","  filtered=rank_in[rank_in.rank_id<=k]\n","  da=pd.merge(filtered,qrels,how='left').fillna(0)\n","\n","  #updating relevant documnt dataframe for final calculation\n","  da['relevance'].values[da['relevance'].values <3 ]=0\n","  da['relevance'].values[da['relevance'].values >=3 ]=1\n","  da['precision']=da['relevance']/da['rank_id']\n","  db=da.loc[da['relevance']==1]\n","\n","  #calculation of MAP\n","  map=db.groupby('q_id', as_index=False)['precision'].mean()['precision'].mean()\n","\n","  return map\n","\n","print ('MAP at top 50 : {0}'.format(calcMAP(50, qrels, rank_in)))\n","print ('MAP at top 100 : {0}'.format(calcMAP(100, qrels, rank_in)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MAP at top 50 : 0.3202907736005067\n","MAP at top 100 : 0.27487083134970786\n"]}]},{"cell_type":"markdown","metadata":{"id":"dFGe-z67KRfY"},"source":["# 7 : Normalized Discounted Cumulative Gain (NDCG) (15 Points)"]},{"cell_type":"markdown","metadata":{"id":"gPBQdRW4KWur"},"source":["Question 7.1 (5 points)\n","\n","Give the definition of NDCG@k corresponding to the top *k* retrieved results for *n* queries (NDCG@k). Use the definition discussed in the lectures. Note that this metric considers graded relevance judgments and you should not binarize the labels. To assign zero gain to non-relevant documents, decrease all relevance labels in the ANTIQUE qrels by 1 point i.e. map relevance judgements 1-4 to 0-3. Please note that you have to use averaging to aggregate the results from all queries."]},{"cell_type":"markdown","metadata":{"id":"6W1VjH8CKa4V"},"source":["**Answer:**\n","\n","The **Discounted Cumulative Gain** for k shown recommendations (DCG@k) sums the relevance of the shown items.\n","\n","The **Normalized Cumulative Gain** for k shown recommendations (NDCG@k) divides this score by the maximum possible value of DCG@k.\n","\n","Here, the maximum possible value of DCG@k would be if the items in the** ranking were sorted by the relevance value(relevance score-1/2/3/4)**.\n","This is called Ideal Discounted Cumulative Gain (IDCG@k).\n","\n","IDCG@k is calculated by sorting the ranking by the true unknown relevance (in descending order) and then use the formula for DCG@k).\n","Here, formula for DCG@k is\n","\n","\n","\n","> DCG@k=∑rel/(log2(i+1))\n","\n","\n","\n","> NDCG@k=(DCG@k)/(IDCG@k)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6LY6e8NDKdWl"},"source":["Question 7.2 (10 points)\n","\n","In the cell below, please enter the code to print the NDCG@k where k={5, 10} for the input ranking file. As mentioned above, please note that the final value is the average of metric values across all queries.\n","\n","Use log base 2 for the calculations.\n"]},{"cell_type":"code","metadata":{"id":"k6z_aExMKiVG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664841311962,"user_tz":240,"elapsed":182,"user":{"displayName":"Harsh Shah","userId":"05727327381693060940"}},"outputId":"60757185-8e65-4b47-a7ce-f9fc9d0069f4"},"source":["\n","'''\n","In this function, calculate NDCG@k, given the input ranking information (rank_in)\n","and the query passage relevance information (qrels).\n","Return Value:\n","ndcg - NDCG@k\n","'''\n","\n","def DCG(k, qrels, rank_in):\n","    #filtering first k documents\n","    rank_filtered=rank_in[rank_in.rank_id<=k]\n","\n","    #processing dataframe for final calculation\n","    da=pd.merge(rank_filtered,qrels,how='left').fillna(0)\n","    da['log2+1']=np.log2(da['rank_id']+1)\n","    da['divide']=da['relevance']/da['log2+1']\n","\n","    #calculation of DCG\n","    db=da.groupby('q_id', as_index=False)['divide'].sum()\n","    DCG=db['divide'].mean()\n","    return DCG\n","\n","def iDCG(k, qrels, rank_in):\n","    #filtering first k documents\n","    rank_filtered=rank_in[rank_in.rank_id<=k]\n","\n","    #processing dataframe for final calculation\n","    da=pd.merge(rank_filtered,qrels,how='left').fillna(0)\n","    da=da.sort_values(by=['q_id','relevance'],ascending=False)\n","    da['index']=(da.groupby('q_id', as_index=False).cumcount())+1\n","    da['log2+1']=np.log2(da['index']+1)\n","    da['divide']=da['relevance']/da['log2+1']\n","\n","    #calculation of iDCG\n","    db=da.groupby('q_id', as_index=False)['divide'].sum()\n","    DCG=db['divide'].mean()\n","    return DCG\n","\n","def calcNDCG(k, qrels, rank_in):\n","  #final calculation of ndcg=dcg/idcg\n","\n","  ndcg=DCG(k, qrels, rank_in)/iDCG(k, qrels, rank_in)\n","  return ndcg\n","\n","print ('NDCG at top 5 : {0}'.format(calcNDCG(5, qrels, rank_in)))\n","print ('NDCG at top 10 : {0}'.format(calcNDCG(10, qrels, rank_in)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NDCG at top 5 : 0.8214802384921778\n","NDCG at top 10 : 0.7539200567798043\n"]}]},{"cell_type":"markdown","source":["References:\n","\n","\n","1.   [Stanford handout](https://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf)\n","2.  [ A Theoretical Analysis of NDCG Ranking Measures by Wang et al.](http://proceedings.mlr.press/v30/Wang13.pdf)\n","\n"],"metadata":{"id":"HQsbzmlXQQeE"}}]}