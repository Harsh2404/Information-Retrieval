{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1HGEQWToAsqGtoYmWmb__vPCCPDmb0BwL","timestamp":1670606579252},{"file_id":"1UKveAVf7U2OSNVdNis6TjK7Plwt0FHfV","timestamp":1657733541098},{"file_id":"1HqrSIM75QAIo0oaRwyy1_3VbDoykBqtl","timestamp":1649628880812},{"file_id":"1JU4m4J3u8qj_qqQRL6TzDjLTQ2RqQs7e","timestamp":1618280833341}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0KKC3O3QiIi4"},"source":["#ENGG*6600: Special Topics in Information Retrieval - Fall 2022\n","##Assignment 7: Learning to Rank (Total : 100 points)\n","\n","**Description**\n","\n","This assignment consists of programming and analytical questions on Learning to Rank models and neural ranking models. Basic proficiency in Python is recommended.  \n","\n","**Instructions**\n","\n","* To start working on the assignment, you would first need to save the notebook to your local Google Drive. For this purpose, you can click on *Copy to Drive* button. You can alternatively click the *Share* button located at the top right corner and click on *Copy Link* under *Get Link* to get a link and copy this notebook to your Google Drive.  \n","\n","*   For questions with descriptive answers, please replace the text in the cell which states \"Enter your answer here!\" with your answer. If you are using mathematical notation in your answers, please define the variables.\n","*   You should implement all the functions yourself and should not use a library or tool for the computation.\n","*   For coding questions, you can add code where it says \"enter code here\" and execute the cell to print the output.\n","* To create the final pdf submission file, execute *Runtime->RunAll* from the menu to re-execute all the cells and then generate a PDF using *File->Print->Save as PDF*. Make sure that the generated PDF contains all the codes and printed outputs before submission.\n","To create the final python submission file, click on File->Download .py.\n","\n","\n","**Submission Details**\n","\n","* Due data: December. 5, 2022 at 11:59 PM (EDT).\n","* The final PDF and python file must be uploaded on CourseLink.\n","* After copying this notebook to your Google Drive, please paste a link to it below. Use the same process given above to generate a link. ***You will not recieve any credit if you don't paste the link!*** Make sure we can access the file.\n","***LINK: *https://colab.research.google.com/drive/1zKzc-C7RBnyjJ1IhYsprh8dlfKgMiyWg?usp=sharing***\n","\n","**Academic Honesty**\n","\n","Please follow the guidelines under the Collaboration and Help section in the first lecture.      "]},{"cell_type":"markdown","metadata":{"id":"JuVqvXU6ijXi"},"source":["# Download input files"]},{"cell_type":"markdown","metadata":{"id":"SMU5d1a-jCG-"},"source":["Please execute the cell below to download the input files."]},{"cell_type":"code","metadata":{"id":"YM1igiBMcIB8"},"source":["\n","import os\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","\n","import os\n","import zipfile\n","import numpy as np\n","\n","download = drive.CreateFile({'id': '11K8r5a_Aj9S4Cpd8k3x76ccBTluNguip'})\n","download.GetContentFile('HW07.zip')\n","\n","with zipfile.ZipFile('HW07.zip', 'r') as zip_file:\n","    zip_file.extractall('./')\n","os.remove('HW07.zip')\n","# We will use hw05 as our working directory\n","os.chdir('HW07')\n","\n","#Setting the input files\n","passage_file = \"antique-collection.tok.clean_kstem\"\n","test_queries_file = \"antique-test-queries.tok.clean_kstem\"\n","train_queries_file = \"antique-train-queries.tok.clean_kstem\"\n","val_queries_file = \"antique-val-queries.tok.clean_kstem\"\n","sample = \"sample.txt\"\n","stopwords_file = \"stopword_INQUERY\"\n","val_baseline_features_file = \"val_baseline_features_top10\"\n","test_baseline_features_file = \"test_baseline_features_top10\"\n","train_baseline_features_file = \"train_baseline_features_top10\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L84x5CFj9Z-L"},"source":["# 1 : Initial Data Setup (30 points)\n","\n","We use collection from the ANTIQUE  [https://arxiv.org/pdf/1905.08957.pdf] dataset for this assignment. As described in the previous assignments, this is a passage retrieval dataset.\n","\n","The description of the input files provided for this assignment is given below.\n","\n","**Collection file**\n","\n","Each row of the file consists of the following information:\n","\n","*passage_id  passage_text*\n","\n","The id and text information is tab separated. The passage text has been pre-processed to remove punctutation, tokenised and stemmed using the Krovetz stemmer. The terms in the passage text can be accessed by splitting the text based on space.\n","\n","**Query files**\n","\n","You are provided with train,validation and test query files.  Each row of the file consists of the following information:\n","\n","*query_id  query_text*\n","\n","The id and text information is tab separated. The query text has been pre-processed to remove punctutation, tokenised and stemmed using the Krovetz stemmer. The terms in the text can be accessed by splitting the text based on space.\n","\n","**Feature files**\n","\n","You are provided with train,validation and test feature files. Each row of the file consists of the following information:\n","\n","*query_id  passage_id relevance_score vsm_score bm25_score*\n","\n","Each row contains features for a (query,passage) pair and is space separated. The relevance_score is the human annotated relevance score. vsm_score and bm25 scores are the relevance scores for the pair corresponding to the two different scoring methods.\n","\n","**Stopwords file**\n","\n","The stopword file contains the list of stopwords. This file has a stopword per line.\n","\n","**Sample file**\n","\n","For this assignment, we use the pyltr Learning to Rank framework from [https://github.com/harshhpareek/pyltr]. The input file to the framework has to be set similar to the sample.txt in the following format:\n","\n","*relevance_score qid:query_id 1:feature1 2:feature2 #docid = passage_id*\n","\n","Each entry is space separated. This file has been provided only for reference to create files of the same format.\n","\n","In the cell below, you have to implement the following:\n","\n","\n","*   Load the query files\n","*   Load the collection\n","*   Load the stopwords\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"8cQBuULrlz1q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670606904176,"user_tz":300,"elapsed":1444,"user":{"displayName":"Harsh Shah","userId":"05727327381693060940"}},"outputId":"5198d12f-9020-4f7d-c6b1-8bac005e596c"},"source":["\n","'''\n","In this function, load the query files into dict\n","Return Variables:\n","queries - dict with qid as key and querytext as value\n","'''\n","def loadQueryFile(filename):\n","    #enter your code here\n","    queries={}\n","    for line in open(filename, encoding=\"utf8\"):\n","      qid,qtext=line.strip().split('\\t')\n","      queries[qid]=qtext\n","    return queries\n","\n","\n","'''\n","In this function, load the collection into dict\n","Return Variables:\n","coll - dict with passage id as key and passage text as value\n","'''\n","def loadCollection(passage_file):\n","    #enter your code here\n","    coll={}\n","    for line in open(passage_file, encoding=\"utf8\"):\n","       pid,ptext = line.strip().split('\\t')\n","       coll[pid]=ptext\n","    return coll\n","\n","'''\n","In this function, load the stopwords into dict\n","Return Variables:\n","stopwords - dict with stopword as key\n","'''\n","def loadStopWords(stopwords_file):\n","    #enter your code here\n","    stopwords={}\n","    for line in open(stopwords_file, encoding=\"utf8\"):\n","      stopwords[line.strip()]=None\n","    return stopwords\n","\n","\n","train_queries = loadQueryFile(train_queries_file)\n","val_queries = loadQueryFile(val_queries_file)\n","test_queries = loadQueryFile(test_queries_file)\n","coll = loadCollection(passage_file)\n","stopwords = loadStopWords(stopwords_file)\n","\n","print('Total Number of train queries: {0}'.format(len(train_queries)))\n","print('Total Number of validation queries: {0}'.format(len(val_queries)))\n","print('Total Number of test queries: {0}'.format(len(test_queries)))\n","print('Total Number of passages in the collection: {0}'.format(len(coll)))\n","print('Total Number of stopwords: {0}'.format(len(stopwords)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Number of train queries: 2226\n","Total Number of validation queries: 200\n","Total Number of test queries: 200\n","Total Number of passages in the collection: 403492\n","Total Number of stopwords: 418\n"]}]},{"cell_type":"markdown","metadata":{"id":"5JmnkC5ypZ6w"},"source":["# 2 : Feature Preparation (40 points)\n","\n","The input feature file consists of two main features : VSM score and bm25 score of query,passage pairs. In this section, you will implement three additional features and use the information to create input feature file which contains the 5 features. The feature file must have the same format as sample.\n","\n","In the cell below, implement the following features:\n","\n","*  Number of unique term overlap between query and passage after excluding stopwords and words with only one character from both.\n","\n","  [ Example = Query : why do a cat headbutt\n","\n","  Passage : cat fight for attention and domination if you show a can of food to my cat he headbutt it.\n","\n","  Number of Overlapped terms for the query/passage pair: 2  ]\n","\n","*  Number of terms in query\n","*  Number of terms in passage\n"]},{"cell_type":"code","metadata":{"id":"1ouDq6xcqzkr"},"source":["'''\n","In this function, create new feature file with additional features in the format required as input\n","Return Variables:\n","There is no return variable. You would create a new feature file \"final_features_file\"\n","One example of the row of the newly created file is\n","\"0 qid:3990512 1:3.5053628162466897 2:10.841493137122296 3:1 4:6 5:112 #docid = 882429_10\"\n","The format of the file is:\n","\"relevance_score qid:query_id 1:feature1 2:feature2 3:feature3 4:feature4 5:feature5 #docid = passage_id\"\n","You can read through the input baseline_features_file, create additional features and\n","add the updated entry into the new file.\n","'''\n","listofsw=list(stopwords.keys())\n","\n","def rmstopwords(string):\n","    querywords = string.split()\n","    resultwords  = [word for word in querywords if ((word.lower() not in listofsw) and (len(word) != 1))]\n","    result = ' '.join(resultwords)\n","    return result\n","\n","def featureCreation(baseline_features_file, stopwords, train_queries, coll, final_features_file):\n","\n","    with open(str(final_features_file),'w') as f:\n","        for line in open(baseline_features_file, encoding=\"utf8\"):\n","            qid,pid,r,vsm,bm25 =line.strip().split(' ')\n","            query=train_queries[qid]\n","            passage=coll[pid]\n","            srquery=rmstopwords(query)\n","            srpassage=rmstopwords(passage)\n","            common=len(set(srquery.split()) &  set(srpassage.split()))\n","            s=\"{} qid:{} 1:{} 2:{} 3:{} 4:{} 5:{} #docid = {}\\n\".format(r,qid,vsm,bm25,common,len(query),len(passage),pid)\n","            f.write(s)\n","\n","\n","\n","featureCreation(train_baseline_features_file, stopwords, train_queries, coll, 'train_features_final')\n","featureCreation(val_baseline_features_file, stopwords, val_queries, coll, 'val_features_final')\n","featureCreation(test_baseline_features_file, stopwords, test_queries, coll, 'test_features_final')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TfQcIDKLyu3l"},"source":["# 3 : Model Training and Evaluation (30 points)\n","\n","In the cell below, the pyltr library is used to train and evaluate ANTIQUE data using LambdaMART model. This takes less than a minute to execute.\n"]},{"cell_type":"code","source":["!pip install --upgrade scikit-learn==0.22"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SXr8gFTm8G5U","executionInfo":{"status":"ok","timestamp":1670607053598,"user_tz":300,"elapsed":8792,"user":{"displayName":"Harsh Shah","userId":"05727327381693060940"}},"outputId":"ceab9ae7-89f4-4f30-9ce1-3ea68ab49360"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit-learn==0.22\n","  Downloading scikit_learn-0.22-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n","\u001b[K     |████████████████████████████████| 7.0 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.22) (1.7.3)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.22) (1.21.6)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.22) (1.2.0)\n","Installing collected packages: scikit-learn\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.0.2\n","    Uninstalling scikit-learn-1.0.2:\n","      Successfully uninstalled scikit-learn-1.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.22 which is incompatible.\n","imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22 which is incompatible.\u001b[0m\n","Successfully installed scikit-learn-0.22\n"]}]},{"cell_type":"code","metadata":{"id":"qzyFs2dTzAgk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670607190323,"user_tz":300,"elapsed":31448,"user":{"displayName":"Harsh Shah","userId":"05727327381693060940"}},"outputId":"5eed3e64-a37b-45c6-b7a7-2cac2e1a44d4"},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import pyltr\n","with open('train_features_final') as trainfile, open('val_features_final') as valifile, open('test_features_final') as evalfile:\n","    TX, Ty, Tqids, _ = pyltr.data.letor.read_dataset(trainfile)\n","    VX, Vy, Vqids, _ = pyltr.data.letor.read_dataset(valifile)\n","    EX, Ey, Eqids, _ = pyltr.data.letor.read_dataset(evalfile)\n","\n","\n","metric = pyltr.metrics.NDCG(k=10)\n","monitor = pyltr.models.monitors.ValidationMonitor(VX, Vy, Vqids, metric=metric, stop_after=250)\n","\n","model = pyltr.models.LambdaMART(\n","    metric=metric,\n","    n_estimators=100,\n","    learning_rate=0.02,\n","    max_features=0.5,\n","    query_subsample=0.5,\n","    max_leaf_nodes=10,\n","    min_samples_leaf=64,\n","    verbose=1,\n",")\n","model.fit(TX, Ty, Tqids,monitor=monitor)\n","Epred = model.predict(EX)\n","print ('LambdaMART model test score :'+ str(metric.calc_mean(Eqids, Ey, Epred)))\n",""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Early termination at iteration  99\n","LambdaMART model test score :0.7976729429247746\n"]}]},{"cell_type":"markdown","source":["Describe how LambdaMART works. A brief description the model and training objective would be sufficient.\n","\n","LambdaMART is a combination of LambdaRank and MART (Multiple Additive Regression Trees). LambdaMART is a technique where ranking is transformed into a pairwise classification or regression problem. The algorithms consider a pair of items at a single time, coming up with a viable ordering of those items before initiating the final order of the entire list.\n","\n","MART uses gradient boosted decision trees for prediction tasks. However, LambdaMART improves this by using gradient boosted decision trees with a cost function derived from LambdaRank to order any ranking situation.\n","\n","Training Objective of Lambdamart:\n","\n","Learning to Rank (LTR)or LambdaMart is a class of techniques that apply supervised machine learning (ML) to solve ranking problems. The main difference between LTR and traditional supervised ML is this:\n","1. Traditional ML solves a prediction problem (classification or regression) on a single instance at a time. E.g. if you are doing spam detection on email, you will look at all the features associated with that email and classify it as spam or not. The aim of traditional ML is to come up with a class (spam or no-spam) or a single numerical score for that instance.\n","2. LTR solves a ranking problem on a list of items. The aim of LTR is to come up with optimal ordering of those items. As such, LTR doesn’t care much about the exact score that each item gets, but cares more about the relative ordering among all the items.\n","\n","The most common application of LTR is search engine ranking, but it’s useful anywhere you need to produce a ranked list of items."],"metadata":{"id":"RwqkbDy8MHKp"}},{"cell_type":"code","source":[],"metadata":{"id":"VARHTrUMxDyK"},"execution_count":null,"outputs":[]}]}
